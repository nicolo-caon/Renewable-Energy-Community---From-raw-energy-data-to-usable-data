# Import all libraries #
from google.colab import files
import zipfile
import os
import glob
import pandas as pd
import numpy as np
from collections import defaultdict

# First upload a folder as zip and extract files, it contains e-distribuzione raw data csv about #
# Clear old data folder before loading new, since we are going to run the code two times to elaborate "immessa" and "prelevata"
extract_folder = '/content/data'

if os.path.exists(extract_folder):
    shutil.rmtree(extract_folder)  
    print(f"Previous data at deleted.")
    
uploaded = files.upload()
zip_path = list(uploaded.keys())[0]   
zip_path = f'/content/{zip_path}'    

extract_folder = '/content/data'

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_folder)

print("Unzipped to:", extract_folder)

# Find all CSV files recursively, set up a dictionary and load all csv #
csv_files = glob.glob(os.path.join(extract_folder, '**', '*.csv'), recursive=True)
print(f"Found {len(csv_files)} CSV files")

# Dictionary
dfs = {}

# Load CSV
for f in csv_files:
    try:
        df = pd.read_csv(
            f,
            sep=';',        # correct delimiter
            decimal=',',    # convert , to . for decimals
            index_col=0,    # first column = row names
            engine='python',
            on_bad_lines='warn'
        )
        dfs[os.path.basename(f)] = df
        print(f"\nLoaded file: {os.path.basename(f)}")
        print(df.head())  # show first 5 rows
    except Exception as e:
        print(f"Failed to read {f}: {e}")

# Loop over each loaded DataFrame to check for NaN, we'll se a pattern #
for filename, df in dfs.items():
    nan_cols = df.columns[df.isna().any()].tolist()
    if nan_cols:
        print(f"File: {filename}")
        print("Columns with NaN:", nan_cols)
        print()

# We want to group Production/consumption (whatever we uploaded) of energy by month, regardless of who consumes/procudes it. (File names include the month they refear to) #

# Group files by month
month_files = defaultdict(list)
for filename in dfs.keys():
    month = filename.split('_')[0]
    month_files[month].append(filename)

# We need to sum all data by month
monthly_sums = {}

for month, files in month_files.items():
    print(f"Processing month: {month} with {len(files)} file(s)")
    month_df = None
    for f in files:
        df = dfs[f]  # get cleaned DataFrame
        if month_df is None:
            month_df = df.copy()
        else:
            # sum cell by cell
            month_df = month_df.add(df, fill_value=0)
    monthly_sums[month] = month_df

# Example: show summed DataFrame for 'maggio'
if 'maggio' in monthly_sums:
    print("\nSummed DataFrame for maggio:")
    print(monthly_sums['maggio'].head())

# We want to generate a single file with all data, ordered cronologically #

all_months_df = pd.concat(monthly_sums.values(), axis=0)

# Convert the index (dates) to datetime, handle dd/mm/yyyy format
all_months_df.index = pd.to_datetime(all_months_df.index, dayfirst=True, errors='coerce')

# Sort rows chronologically by date
all_months_df = all_months_df.sort_index()

print("Final concatenated 15-min DataFrame:")
print(all_months_df.head())

# We have to aggregate 15 min data to hourly data #

def sum_15min_to_hour(df):
    cols = df.columns.tolist()
    hourly_sums = []
    for i in range(0, len(cols), 4):
        group = cols[i:i+4]
        hourly_sum = df[group].sum(axis=1)
        hourly_sums.append(hourly_sum)

    hourly_df = pd.concat(hourly_sums, axis=1)
    hourly_df.columns = [f'Hour {i+1}' for i in range(len(hourly_df.columns))]
    return hourly_df

# Apply to all monthly_sums
monthly_sums_hourly = {}
for month, df in monthly_sums.items():
    monthly_sums_hourly[month] = sum_15min_to_hour(df)
    print(f"Hourly summed DataFrame for {month}:")
    print(monthly_sums_hourly[month].head())

all_months_hourly_df = pd.concat(monthly_sums_hourly.values(), axis=0)

# Convert index (dates) to datetime and sort
all_months_hourly_df.index = pd.to_datetime(all_months_hourly_df.index, dayfirst=True, errors='coerce')
all_months_hourly_df = all_months_hourly_df.loc[all_months_hourly_df.index.notna()]
all_months_hourly_df = all_months_hourly_df.sort_index()

print("Final concatenated hourly DataFrame:")
print(all_months_hourly_df.head())

# Than we can save is the way we want, I need an excel file #
from google.colab import files

# Convert index to datetime
all_months_hourly_df.index = pd.to_datetime(all_months_hourly_df.index, dayfirst=True)

# Format index as dd/mm/yyyy strings for readability in Excel
all_months_hourly_df.index = all_months_hourly_df.index.strftime('%d/%m/%Y')
all_months_hourly_df.index.name = 'Date'

# Save to Excel
output_excel = '/content/final_hourly_prelevata.xlsx'
all_months_hourly_df.to_excel(output_excel, index=True)

# Trigger download in Colab
files.download(output_excel)

# Repeat the process for "immessa", the other folder, changing the zip file #
############################################################################################

# The only thing left to do is to fin the hourly minimum between "immessa" and "prelevata" #
# Uploading le files through a zip folder like I did erlyer
extract_folder = '/content/data'

if os.path.exists(extract_folder):
    shutil.rmtree(extract_folder)  # delete old extracted data
    print(f"Previous data at {extract_folder} deleted.")

uploaded = files.upload()
zip_path = list(uploaded.keys())[0]  
zip_path = f'/content/{zip_path}'    

extract_folder = '/content/data'

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_folder)

print("Unzipped to:", extract_folder)

# Detecting data
prelevata_path = None
immessa_path = None

for root, dirs, files_list in os.walk('/content/data'):
    for f in files_list:
        lower_f = f.lower()
        if 'prel' in lower_f and lower_f.endswith('.csv'):
            prelevata_path = os.path.join(root, f)
        elif 'imm' in lower_f and lower_f.endswith('.csv'):
            immessa_path = os.path.join(root, f)

print("Detected prelevata file:", prelevata_path)
print("Detected immessa file:", immessa_path)

# Correct reading (comma=separator, dot=decimal)
prelevata = pd.read_csv(prelevata_path, sep=',', decimal='.', index_col='Date', parse_dates=True)
immessa   = pd.read_csv(immessa_path, sep=',', decimal='.', index_col='Date', parse_dates=True)

# Ensure all values are numeric
prelevata = prelevata.apply(pd.to_numeric, errors='coerce')
immessa = immessa.apply(pd.to_numeric, errors='coerce')

# Compute aligned elementwise minimum, ignoring NaN
min_table = prelevata.combine(immessa, np.fmin)

print(prelevata.head())
print(immessa.head())
print(min_table.head())

# Save the min file as an excel file
output_path = "/content/min_table.xlsx"

with pd.ExcelWriter(output_path, engine="openpyxl", date_format="DD/MM/YYYY") as writer:
    min_table.to_excel(writer, sheet_name="Min Values")

# Trigger browser download
files.download(output_path)
